{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Task Management System\n",
    "## Week 3: Priority Prediction and Workload Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('../data/cleaned_tasks.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for Priority Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le_category = LabelEncoder()\n",
    "le_assigned_to = LabelEncoder()\n",
    "le_status = LabelEncoder()\n",
    "le_priority = LabelEncoder()\n",
    "\n",
    "df['category_encoded'] = le_category.fit_transform(df['category'])\n",
    "df['assigned_to_encoded'] = le_assigned_to.fit_transform(df['assigned_to'])\n",
    "df['status_encoded'] = le_status.fit_transform(df['status'])\n",
    "df['priority_encoded'] = le_priority.fit_transform(df['priority'])\n",
    "\n",
    "# TF-IDF features for descriptions\n",
    "tfidf = TfidfVectorizer(max_features=500, ngram_range=(1, 2))\n",
    "tfidf_features = tfidf.fit_transform(df['processed_description'])\n",
    "\n",
    "# Combine all features\n",
    "numerical_features = df[['estimated_hours', 'category_encoded', 'assigned_to_encoded', 'status_encoded']].values\n",
    "X_combined = np.hstack((tfidf_features.toarray(), numerical_features))\n",
    "y_priority = df['priority_encoded']\n",
    "\n",
    "print(\"Combined feature shape:\", X_combined.shape)\n",
    "print(\"Priority classes:\", le_priority.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_priority, test_size=0.2, random_state=42, stratify=y_priority)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest for Priority Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Random Forest parameters:\", rf_grid.best_params_)\n",
    "print(\"Best Random Forest CV score:\", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Random Forest\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "\n",
    "print(\"\\nRandom Forest Test Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n",
    "print(\"Precision:\", precision_score(y_test, rf_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, rf_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, rf_pred, average='weighted'))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, rf_pred, target_names=le_priority.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, rf_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le_priority.classes_, yticklabels=le_priority.classes_)\n",
    "plt.title('Confusion Matrix - Random Forest (Priority Prediction)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost for Priority Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with GridSearchCV\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb.XGBClassifier(random_state=42), xgb_param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGBoost parameters:\", xgb_grid.best_params_)\n",
    "print(\"Best XGBoost CV score:\", xgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate XGBoost\n",
    "xgb_best = xgb_grid.best_estimator_\n",
    "xgb_pred = xgb_best.predict(X_test)\n",
    "\n",
    "print(\"\\nXGBoost Test Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, xgb_pred))\n",
    "print(\"Precision:\", precision_score(y_test, xgb_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, xgb_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, xgb_pred, average='weighted'))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, xgb_pred, target_names=le_priority.classes_))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, xgb_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le_priority.classes_, yticklabels=le_priority.classes_)\n",
    "plt.title('Confusion Matrix - XGBoost (Priority Prediction)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Balancing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate current workload for each assignee\n",
    "workload = df.groupby('assigned_to').agg({\n",
    "    'estimated_hours': 'sum',\n",
    "    'task_id': 'count',\n",
    "    'priority': lambda x: (x == 'High').sum()\n",
    "}).rename(columns={'task_id': 'task_count', 'priority': 'high_priority_count'})\n",
    "\n",
    "print(\"Current Workload Summary:\")\n",
    "workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize workload distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "workload['estimated_hours'].plot(kind='bar', ax=axes[0], color='skyblue')\n",
    "axes[0].set_title('Total Estimated Hours by Assignee')\n",
    "axes[0].set_xlabel('Assignee')\n",
    "axes[0].set_ylabel('Estimated Hours')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "workload['task_count'].plot(kind='bar', ax=axes[1], color='lightgreen')\n",
    "axes[1].set_title('Task Count by Assignee')\n",
    "axes[1].set_xlabel('Assignee')\n",
    "axes[1].set_ylabel('Task Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "workload['high_priority_count'].plot(kind='bar', ax=axes[2], color='salmon')\n",
    "axes[2].set_title('High Priority Tasks by Assignee')\n",
    "axes[2].set_xlabel('Assignee')\n",
    "axes[2].set_ylabel('High Priority Count')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heuristic workload balancing function\n",
    "def balance_workload(df, max_hours_per_person=40, max_tasks_per_person=10):\n",
    "    \"\"\"\n",
    "    Balance workload using heuristic approach\n",
    "    \"\"\"\n",
    "    balanced_df = df.copy()\n",
    "    assignees = df['assigned_to'].unique()\n",
    "    \n",
    "    # Calculate current workload\n",
    "    current_workload = df.groupby('assigned_to').agg({\n",
    "        'estimated_hours': 'sum',\n",
    "        'task_id': 'count'\n",
    "    }).rename(columns={'task_id': 'task_count'})\n",
    "    \n",
    "    # Find overloaded assignees\n",
    "    overloaded = current_workload[\n",
    "        (current_workload['estimated_hours'] > max_hours_per_person) | \n",
    "        (current_workload['task_count'] > max_tasks_per_person)\n",
    "    ]\n",
    "    \n",
    "    print(\"Overloaded assignees:\")\n",
    "    print(overloaded)\n",
    "    \n",
    "    # Simple rebalancing: move tasks from overloaded to underloaded assignees\n",
    "    underloaded = current_workload[\n",
    "        (current_workload['estimated_hours'] <= max_hours_per_person) & \n",
    "        (current_workload['task_count'] <= max_tasks_per_person)\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nUnderloaded assignees:\")\n",
    "    print(underloaded)\n",
    "    \n",
    "    # For demonstration, we'll randomly reassign some tasks\n",
    "    # In a real scenario, this would be more sophisticated\n",
    "    if len(overloaded) > 0 and len(underloaded) > 0:\n",
    "        overloaded_tasks = balanced_df[balanced_df['assigned_to'].isin(overloaded.index)]\n",
    "        underloaded_assignees = underloaded.index.tolist()\n",
    "        \n",
    "        # Reassign 20% of overloaded tasks to underloaded assignees\n",
    "        reassign_count = int(len(overloaded_tasks) * 0.2)\n",
    "        reassign_indices = np.random.choice(overloaded_tasks.index, reassign_count, replace=False)\n",
    "        \n",
    "        for idx in reassign_indices:\n",
    "            new_assignee = np.random.choice(underloaded_assignees)\n",
    "            balanced_df.loc[idx, 'assigned_to'] = new_assignee\n",
    "            print(f\"Reassigned task {balanced_df.loc[idx, 'task_id']} to {new_assignee}\")\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "# Apply workload balancing\n",
    "balanced_df = balance_workload(df)\n",
    "\n",
    "# Recalculate workload after balancing\n",
    "balanced_workload = balanced_df.groupby('assigned_to').agg({\n",
    "    'estimated_hours': 'sum',\n",
    "    'task_id': 'count',\n",
    "    'priority': lambda x: (x == 'High').sum()\n",
    "}).rename(columns={'task_id': 'task_count', 'priority': 'high_priority_count'})\n",
    "\n",
    "print(\"\\nBalanced Workload Summary:\")\n",
    "balanced_workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "# Assuming XGBoost performed better\n",
    "joblib.dump(xgb_best, '../models/priority_predictor.pkl')\n",
    "joblib.dump(le_priority, '../models/priority_encoder.pkl')\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer_priority.pkl')\n",
    "joblib.dump(le_category, '../models/category_encoder.pkl')\n",
    "joblib.dump(le_assigned_to, '../models/assignee_encoder.pkl')\n",
    "joblib.dump(le_status, '../models/status_encoder.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "\n",
    "# Save balanced dataset\n",
    "balanced_df.to_csv('../data/balanced_tasks.csv', index=False)\n",
    "print(\"Balanced dataset saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "nbconvert_exporter",
   "python": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}