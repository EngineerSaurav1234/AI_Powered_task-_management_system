{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Task Management System\n",
    "## Week 1: Exploratory Data Analysis (EDA) and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/tasks.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the dataset\n",
    "print(\"Dataset info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Convert due_date to datetime\n",
    "df['due_date'] = pd.to_datetime(df['due_date'])\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Duplicate rows:\", df.duplicated().sum())\n",
    "\n",
    "# Handle any missing values (if any)\n",
    "# For this dataset, we'll assume no missing values, but in real scenarios:\n",
    "# df['estimated_hours'].fillna(df['estimated_hours'].median(), inplace=True)\n",
    "\n",
    "print(\"Data types after cleaning:\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of priorities\n",
    "plt.figure(figsize=(10, 6))\n",
    "priority_counts = df['priority'].value_counts()\n",
    "plt.pie(priority_counts.values, labels=priority_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Distribution of Task Priorities')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of statuses\n",
    "plt.figure(figsize=(12, 6))\n",
    "status_counts = df['status'].value_counts()\n",
    "sns.barplot(x=status_counts.index, y=status_counts.values)\n",
    "plt.title('Distribution of Task Statuses')\n",
    "plt.xlabel('Status')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "category_counts = df['category'].value_counts()\n",
    "sns.barplot(x=category_counts.index, y=category_counts.values)\n",
    "plt.title('Distribution of Task Categories')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated hours distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['estimated_hours'], bins=10, kde=True)\n",
    "plt.title('Distribution of Estimated Hours')\n",
    "plt.xlabel('Estimated Hours')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks by assignee\n",
    "plt.figure(figsize=(12, 6))\n",
    "assignee_counts = df['assigned_to'].value_counts()\n",
    "sns.barplot(x=assignee_counts.index, y=assignee_counts.values)\n",
    "plt.title('Tasks Assigned to Each Person')\n",
    "plt.xlabel('Assignee')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for numerical columns\n",
    "plt.figure(figsize=(8, 6))\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Preprocessing on Task Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLTK tools\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and stem\n",
    "    filtered_tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply preprocessing to task descriptions\n",
    "df['processed_description'] = df['task_description'].apply(preprocess_text)\n",
    "\n",
    "print(\"Original vs Processed descriptions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Original: {df['task_description'][i]}\")\n",
    "    print(f\"Processed: {df['processed_description'][i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud of task descriptions\n",
    "all_descriptions = ' '.join(df['processed_description'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Task Descriptions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and preprocessed dataset\n",
    "df.to_csv('../data/cleaned_tasks.csv', index=False)\n",
    "print(\"Cleaned dataset saved to '../data/cleaned_tasks.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
