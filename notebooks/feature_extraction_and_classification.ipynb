{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Task Management System\n",
    "## Week 2: Feature Extraction and Task Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "df = pd.read_csv('../data/cleaned_tasks.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['processed_description'])\n",
    "\n",
    "print(\"TF-IDF feature shape:\", X_tfidf.shape)\n",
    "print(\"Top 20 features:\")\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(feature_names[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification using Naive Bayes and SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare target variables\n",
    "le_priority = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_status = LabelEncoder()\n",
    "\n",
    "y_priority = le_priority.fit_transform(df['priority'])\n",
    "y_category = le_category.fit_transform(df['category'])\n",
    "y_status = le_status.fit_transform(df['status'])\n",
    "\n",
    "print(\"Priority classes:\", le_priority.classes_)\n",
    "print(\"Category classes:\", le_category.classes_)\n",
    "print(\"Status classes:\", le_status.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate models\n",
    "def train_evaluate_model(X, y, model, model_name, target_name):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"\\n{model_name} - {target_name} Classification Results:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name} ({target_name})')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes for Priority Classification\n",
    "nb_priority = MultinomialNB()\n",
    "nb_priority_model, nb_acc, nb_prec, nb_rec, nb_f1 = train_evaluate_model(X_tfidf, y_priority, nb_priority, \"Naive Bayes\", \"Priority\")\n",
    "\n",
    "# Train SVM for Priority Classification\n",
    "svm_priority = SVC(kernel='linear', random_state=42)\n",
    "svm_priority_model, svm_acc, svm_prec, svm_rec, svm_f1 = train_evaluate_model(X_tfidf, y_priority, svm_priority, \"SVM\", \"Priority\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes for Category Classification\n",
    "nb_category = MultinomialNB()\n",
    "nb_category_model, nb_cat_acc, nb_cat_prec, nb_cat_rec, nb_cat_f1 = train_evaluate_model(X_tfidf, y_category, nb_category, \"Naive Bayes\", \"Category\")\n",
    "\n",
    "# Train SVM for Category Classification\n",
    "svm_category = SVC(kernel='linear', random_state=42)\n",
    "svm_category_model, svm_cat_acc, svm_cat_prec, svm_cat_rec, svm_cat_f1 = train_evaluate_model(X_tfidf, y_category, svm_category, \"SVM\", \"Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation scores\n",
    "def cross_validate_model(X, y, model, model_name, target_name):\n",
    "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "    print(f\"\\n{model_name} - {target_name} Cross-Validation Results:\")\n",
    "    print(f\"CV Accuracy Scores: {cv_scores}\")\n",
    "    print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    return cv_scores\n",
    "\n",
    "# Cross-validate priority models\n",
    "nb_priority_cv = cross_validate_model(X_tfidf, y_priority, MultinomialNB(), \"Naive Bayes\", \"Priority\")\n",
    "svm_priority_cv = cross_validate_model(X_tfidf, y_priority, SVC(kernel='linear', random_state=42), \"SVM\", \"Priority\")\n",
    "\n",
    "# Cross-validate category models\n",
    "nb_category_cv = cross_validate_model(X_tfidf, y_category, MultinomialNB(), \"Naive Bayes\", \"Category\")\n",
    "svm_category_cv = cross_validate_model(X_tfidf, y_category, SVC(kernel='linear', random_state=42), \"SVM\", \"Category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best models\n",
    "# For priority prediction, let's assume SVM performed better\n",
    "joblib.dump(svm_priority_model, '../models/priority_classifier.pkl')\n",
    "joblib.dump(svm_category_model, '../models/category_classifier.pkl')\n",
    "joblib.dump(tfidf_vectorizer, '../models/tfidf_vectorizer.pkl')\n",
    "joblib.dump(le_priority, '../models/priority_encoder.pkl')\n",
    "joblib.dump(le_category, '../models/category_encoder.pkl')\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table\n",
    "performance_summary = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes (Priority)', 'SVM (Priority)', 'Naive Bayes (Category)', 'SVM (Category)'],\n",
    "    'Accuracy': [nb_acc, svm_acc, nb_cat_acc, svm_cat_acc],\n",
    "    'Precision': [nb_prec, svm_prec, nb_cat_prec, svm_cat_prec],\n",
    "    'Recall': [nb_rec, svm_rec, nb_cat_rec, svm_cat_rec],\n",
    "    'F1-Score': [nb_f1, svm_f1, nb_cat_f1, svm_cat_f1]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Summary:\")\n",
    "performance_summary.round(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "nbconvert_exporter",
   "python": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}